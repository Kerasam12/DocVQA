Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/beit/feature_extraction_beit.py:28: FutureWarning: The class BeitFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use BeitImageProcessor instead.
  warnings.warn(
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/beit/image_processing_beit.py:110: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
epoch: 0
It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.
tensor(3.9427, device='cuda:0', grad_fn=<NllLossBackward0>)
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
tensor(4.0682, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.2586, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.9991, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.3487, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.4819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.8142, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.7271, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.0334, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.9339, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.2380, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.8694, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.6830, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.7509, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.4017, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.9005, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.8114, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.7843, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.2533, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(6.4350, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.8018, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.1424, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.6394, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.7972, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.1324, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.1663, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.4042, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.7305, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.7925, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.1969, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.4026, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.0844, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.2178, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.5553, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.3574, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.6401, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.8819, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.8050, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.8141, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.1707, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.7452, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(7.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.3280, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(6.1617, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.1376, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.8818, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(2.8084, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.4771, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.9168, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.7227, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(4.0118, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.7664, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(3.8180, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(5.9887, device='cuda:0', grad_fn=<NllLossBackward0>)
