Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/beit/feature_extraction_beit.py:28: FutureWarning: The class BeitFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use BeitImageProcessor instead.
  warnings.warn(
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/models/beit/image_processing_beit.py:110: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
epoch: 0
130 160
112 178
31 259
97 193
29 261
38 252
78 212
56 234
46 244
67 223
75 215
29 261
37 253
40 250
It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.
48 242
70 220
tensor(6.3555, device='cuda:0', grad_fn=<NllLossBackward0>)
21 269
101 189
/home/jsamper/anaconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
132 158
40 250
77 213
171 119
70 220
53 237
37 253
19 271
126 164
39 251
81 209
115 175
21 269
19 271
tensor(6.1593, device='cuda:0', grad_fn=<NllLossBackward0>)
54 236
75 215
11 279
73 217
130 160
19 271
18 272
34 256
23 267
77 213
12 278
45 245
35 255
42 248
11 279
35 255
tensor(6.4784, device='cuda:0', grad_fn=<NllLossBackward0>)
8 282
76 214
10 280
7 283
61 229
50 240
27 263
39 251
42 248
49 241
77 213
37 253
43 247
68 222
58 232
103 187
tensor(2.5736, device='cuda:0', grad_fn=<NllLossBackward0>)
71 219
66 224
10 280
269 21
28 262
74 216
13 277
12 278
68 222
209 81
17 273
54 236
46 244
33 257
44 246
36 254
tensor(5.8130, device='cuda:0', grad_fn=<NllLossBackward0>)
31 259
48 242
73 217
50 240
36 254
120 170
62 228
35 255
12 278
41 249
68 222
35 255
98 192
38 252
93 197
39 251
tensor(4.3620, device='cuda:0', grad_fn=<NllLossBackward0>)
45 245
67 223
18 272
96 194
22 268
62 228
39 251
89 201
47 243
22 268
37 253
49 241
149 141
58 232
114 176
82 208
tensor(5.3033, device='cuda:0', grad_fn=<NllLossBackward0>)
12 278
73 217
79 211
56 234
33 257
108 182
35 255
22 268
38 252
27 263
43 247
45 245
93 197
66 224
28 262
63 227
tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)
105 185
99 191
42 248
110 180
46 244
20 270
24 266
140 150
79 211
27 263
37 253
110 180
45 245
64 226
31 259
41 249
tensor(3.7552, device='cuda:0', grad_fn=<NllLossBackward0>)
18 272
26 264
79 211
24 266
37 253
42 248
19 271
36 254
68 222
47 243
26 264
14 276
23 267
51 239
63 227
35 255
tensor(4.1315, device='cuda:0', grad_fn=<NllLossBackward0>)
95 195
127 163
25 265
20 270
29 261
45 245
158 132
26 264
96 194
125 165
8 282
43 247
20 270
41 249
62 228
26 264
tensor(4.6877, device='cuda:0', grad_fn=<NllLossBackward0>)
27 263
35 255
70 220
68 222
43 247
89 201
34 256
76 214
42 248
